# max_tokens:
# The maximum number of tokens that can be generated in the completion.

# The token count of your prompt plus max_tokens cannot exceed the model's context length.